{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WDSI.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4fvTqXMEO3B"
      },
      "source": [
        "# **Wprowadzenie do sztucznej inteligencji**\n",
        "### Tomasz Gruzdzis 171898\n",
        "### Krzysztof Wicki 171703\n",
        "\n",
        "\n",
        "Katedra Inyżnierii Biomedyczna,\n",
        "Wydział Elektroniki, Telekomunikacji i Informatyki\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezWLG9G0ERSw"
      },
      "source": [
        "## 1. Import bibliotek"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITHIOOawELYd"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn import preprocessing\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import RFE, RFECV\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint as sp_randint\n",
        "from sklearn.svm import SVC, LinearSVC\n",
        "\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDDtbxFvGaCa"
      },
      "source": [
        "## 2. Odczyt danych"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "Ilz4uXZtGYMg",
        "outputId": "76a852a8-0b7b-4722-cf0b-46769319fb14"
      },
      "source": [
        "df = pd.read_csv('arrhythmia.data', header=None, na_values = '?')\n",
        "print(df.info())\n",
        "df.head(5)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 452 entries, 0 to 451\n",
            "Columns: 280 entries, 0 to 279\n",
            "dtypes: float64(125), int64(155)\n",
            "memory usage: 988.9 KB\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>240</th>\n",
              "      <th>241</th>\n",
              "      <th>242</th>\n",
              "      <th>243</th>\n",
              "      <th>244</th>\n",
              "      <th>245</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "      <th>259</th>\n",
              "      <th>260</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>75</td>\n",
              "      <td>0</td>\n",
              "      <td>190</td>\n",
              "      <td>80</td>\n",
              "      <td>91</td>\n",
              "      <td>193</td>\n",
              "      <td>371</td>\n",
              "      <td>174</td>\n",
              "      <td>121</td>\n",
              "      <td>-16</td>\n",
              "      <td>13.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>63.0</td>\n",
              "      <td>0</td>\n",
              "      <td>52</td>\n",
              "      <td>44</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>44</td>\n",
              "      <td>20</td>\n",
              "      <td>36</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>52</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.4</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>5.9</td>\n",
              "      <td>-3.9</td>\n",
              "      <td>52.7</td>\n",
              "      <td>-0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>-8.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>5.1</td>\n",
              "      <td>17.7</td>\n",
              "      <td>70.7</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.5</td>\n",
              "      <td>-4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>3.9</td>\n",
              "      <td>25.5</td>\n",
              "      <td>62.9</td>\n",
              "      <td>-0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>-0.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>2.9</td>\n",
              "      <td>23.3</td>\n",
              "      <td>49.4</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>165</td>\n",
              "      <td>64</td>\n",
              "      <td>81</td>\n",
              "      <td>174</td>\n",
              "      <td>401</td>\n",
              "      <td>149</td>\n",
              "      <td>39</td>\n",
              "      <td>25</td>\n",
              "      <td>37.0</td>\n",
              "      <td>-17.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>53.0</td>\n",
              "      <td>0</td>\n",
              "      <td>48</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.8</td>\n",
              "      <td>-7.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>3.8</td>\n",
              "      <td>-5.7</td>\n",
              "      <td>27.7</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.5</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2.6</td>\n",
              "      <td>11.8</td>\n",
              "      <td>34.6</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>-2.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>2.6</td>\n",
              "      <td>21.6</td>\n",
              "      <td>43.4</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>2.1</td>\n",
              "      <td>20.4</td>\n",
              "      <td>38.8</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>54</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>95</td>\n",
              "      <td>138</td>\n",
              "      <td>163</td>\n",
              "      <td>386</td>\n",
              "      <td>185</td>\n",
              "      <td>102</td>\n",
              "      <td>96</td>\n",
              "      <td>34.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>80</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>56</td>\n",
              "      <td>52</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.8</td>\n",
              "      <td>-4.1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.3</td>\n",
              "      <td>20.4</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-5.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2.2</td>\n",
              "      <td>-3.0</td>\n",
              "      <td>20.7</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.1</td>\n",
              "      <td>-3.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>3.4</td>\n",
              "      <td>11.5</td>\n",
              "      <td>48.2</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.5</td>\n",
              "      <td>-2.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>3.4</td>\n",
              "      <td>12.3</td>\n",
              "      <td>49.0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>55</td>\n",
              "      <td>0</td>\n",
              "      <td>175</td>\n",
              "      <td>94</td>\n",
              "      <td>100</td>\n",
              "      <td>202</td>\n",
              "      <td>380</td>\n",
              "      <td>179</td>\n",
              "      <td>143</td>\n",
              "      <td>28</td>\n",
              "      <td>11.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>71.0</td>\n",
              "      <td>0</td>\n",
              "      <td>72</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>48</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>64</td>\n",
              "      <td>36</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>36</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>-7.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>4.1</td>\n",
              "      <td>7.6</td>\n",
              "      <td>51.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>-5.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>3.3</td>\n",
              "      <td>28.8</td>\n",
              "      <td>63.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>-3.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>3.0</td>\n",
              "      <td>36.8</td>\n",
              "      <td>68.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.2</td>\n",
              "      <td>-2.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>2.6</td>\n",
              "      <td>34.6</td>\n",
              "      <td>61.6</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>75</td>\n",
              "      <td>0</td>\n",
              "      <td>190</td>\n",
              "      <td>80</td>\n",
              "      <td>88</td>\n",
              "      <td>181</td>\n",
              "      <td>360</td>\n",
              "      <td>177</td>\n",
              "      <td>103</td>\n",
              "      <td>-16</td>\n",
              "      <td>13.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>48</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>52</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.5</td>\n",
              "      <td>-10.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>4.7</td>\n",
              "      <td>-4.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>-7.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>4.9</td>\n",
              "      <td>16.2</td>\n",
              "      <td>63.2</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.1</td>\n",
              "      <td>-0.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>2.9</td>\n",
              "      <td>21.7</td>\n",
              "      <td>48.9</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.1</td>\n",
              "      <td>-3.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>3.9</td>\n",
              "      <td>25.4</td>\n",
              "      <td>62.8</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 280 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   0    1    2    3    4    5    6    ...  273  274  275  276   277   278  279\n",
              "0   75    0  190   80   91  193  371  ...  0.0  0.0  0.9  2.9  23.3  49.4    8\n",
              "1   56    1  165   64   81  174  401  ...  0.0  0.0  0.2  2.1  20.4  38.8    6\n",
              "2   54    0  172   95  138  163  386  ...  0.0  0.0  0.3  3.4  12.3  49.0   10\n",
              "3   55    0  175   94  100  202  380  ...  0.0  0.0  0.4  2.6  34.6  61.6    1\n",
              "4   75    0  190   80   88  181  360  ...  0.0  0.0 -0.1  3.9  25.4  62.8    7\n",
              "\n",
              "[5 rows x 280 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "5uao9SWkJEiR",
        "outputId": "6c597d94-578e-49c4-d50b-4680296112ae"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>240</th>\n",
              "      <th>241</th>\n",
              "      <th>242</th>\n",
              "      <th>243</th>\n",
              "      <th>244</th>\n",
              "      <th>245</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "      <th>259</th>\n",
              "      <th>260</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>444.000000</td>\n",
              "      <td>430.000000</td>\n",
              "      <td>451.000000</td>\n",
              "      <td>76.000000</td>\n",
              "      <td>451.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.0</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.00000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.0</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.0</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "      <td>452.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>46.471239</td>\n",
              "      <td>0.550885</td>\n",
              "      <td>166.188053</td>\n",
              "      <td>68.170354</td>\n",
              "      <td>88.920354</td>\n",
              "      <td>155.152655</td>\n",
              "      <td>367.207965</td>\n",
              "      <td>169.949115</td>\n",
              "      <td>90.004425</td>\n",
              "      <td>33.676991</td>\n",
              "      <td>36.150901</td>\n",
              "      <td>48.913953</td>\n",
              "      <td>36.716186</td>\n",
              "      <td>-13.592105</td>\n",
              "      <td>74.463415</td>\n",
              "      <td>5.628319</td>\n",
              "      <td>51.628319</td>\n",
              "      <td>20.920354</td>\n",
              "      <td>0.141593</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.035398</td>\n",
              "      <td>0.002212</td>\n",
              "      <td>0.011062</td>\n",
              "      <td>0.011062</td>\n",
              "      <td>0.004425</td>\n",
              "      <td>0.004425</td>\n",
              "      <td>0.008850</td>\n",
              "      <td>5.619469</td>\n",
              "      <td>54.336283</td>\n",
              "      <td>20.59292</td>\n",
              "      <td>0.433628</td>\n",
              "      <td>0.150442</td>\n",
              "      <td>31.637168</td>\n",
              "      <td>0.017699</td>\n",
              "      <td>0.028761</td>\n",
              "      <td>0.002212</td>\n",
              "      <td>0.004425</td>\n",
              "      <td>0.004425</td>\n",
              "      <td>0.015487</td>\n",
              "      <td>16.026549</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.653540</td>\n",
              "      <td>8.039602</td>\n",
              "      <td>-10.150664</td>\n",
              "      <td>0.032965</td>\n",
              "      <td>-0.013496</td>\n",
              "      <td>0.226770</td>\n",
              "      <td>3.894690</td>\n",
              "      <td>-8.269027</td>\n",
              "      <td>32.422788</td>\n",
              "      <td>0.001106</td>\n",
              "      <td>-0.297566</td>\n",
              "      <td>11.839381</td>\n",
              "      <td>-7.034513</td>\n",
              "      <td>0.025664</td>\n",
              "      <td>-0.002876</td>\n",
              "      <td>0.547788</td>\n",
              "      <td>2.535841</td>\n",
              "      <td>10.081195</td>\n",
              "      <td>33.328540</td>\n",
              "      <td>-0.285398</td>\n",
              "      <td>-0.277212</td>\n",
              "      <td>11.369912</td>\n",
              "      <td>-3.607522</td>\n",
              "      <td>0.016814</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.546681</td>\n",
              "      <td>1.722124</td>\n",
              "      <td>17.840044</td>\n",
              "      <td>32.871460</td>\n",
              "      <td>-0.302434</td>\n",
              "      <td>-0.278982</td>\n",
              "      <td>9.048009</td>\n",
              "      <td>-1.457301</td>\n",
              "      <td>0.003982</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.514823</td>\n",
              "      <td>1.222345</td>\n",
              "      <td>19.326106</td>\n",
              "      <td>29.473230</td>\n",
              "      <td>3.880531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>16.466631</td>\n",
              "      <td>0.497955</td>\n",
              "      <td>37.170340</td>\n",
              "      <td>16.590803</td>\n",
              "      <td>15.364394</td>\n",
              "      <td>44.842283</td>\n",
              "      <td>33.385421</td>\n",
              "      <td>35.633072</td>\n",
              "      <td>25.826643</td>\n",
              "      <td>45.431434</td>\n",
              "      <td>57.858255</td>\n",
              "      <td>29.346409</td>\n",
              "      <td>36.020725</td>\n",
              "      <td>127.220248</td>\n",
              "      <td>13.870684</td>\n",
              "      <td>10.650001</td>\n",
              "      <td>18.249901</td>\n",
              "      <td>20.541728</td>\n",
              "      <td>1.569483</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.046393</td>\n",
              "      <td>0.047036</td>\n",
              "      <td>0.104708</td>\n",
              "      <td>0.104708</td>\n",
              "      <td>0.066445</td>\n",
              "      <td>0.066445</td>\n",
              "      <td>0.093759</td>\n",
              "      <td>11.220680</td>\n",
              "      <td>17.248213</td>\n",
              "      <td>21.06105</td>\n",
              "      <td>3.093161</td>\n",
              "      <td>2.692591</td>\n",
              "      <td>9.624951</td>\n",
              "      <td>0.132002</td>\n",
              "      <td>0.167319</td>\n",
              "      <td>0.047036</td>\n",
              "      <td>0.066445</td>\n",
              "      <td>0.066445</td>\n",
              "      <td>0.123615</td>\n",
              "      <td>21.906457</td>\n",
              "      <td>...</td>\n",
              "      <td>3.414085</td>\n",
              "      <td>5.279719</td>\n",
              "      <td>7.066568</td>\n",
              "      <td>0.390403</td>\n",
              "      <td>0.264398</td>\n",
              "      <td>0.548988</td>\n",
              "      <td>2.990809</td>\n",
              "      <td>32.157008</td>\n",
              "      <td>37.362289</td>\n",
              "      <td>1.015566</td>\n",
              "      <td>1.758544</td>\n",
              "      <td>5.917391</td>\n",
              "      <td>5.061472</td>\n",
              "      <td>0.166763</td>\n",
              "      <td>0.046287</td>\n",
              "      <td>0.426941</td>\n",
              "      <td>2.429776</td>\n",
              "      <td>25.074695</td>\n",
              "      <td>34.361665</td>\n",
              "      <td>0.675060</td>\n",
              "      <td>0.992472</td>\n",
              "      <td>4.793656</td>\n",
              "      <td>2.850633</td>\n",
              "      <td>0.275907</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.370548</td>\n",
              "      <td>1.708190</td>\n",
              "      <td>16.445472</td>\n",
              "      <td>24.421643</td>\n",
              "      <td>0.603551</td>\n",
              "      <td>0.548876</td>\n",
              "      <td>3.472862</td>\n",
              "      <td>2.002430</td>\n",
              "      <td>0.050118</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.347531</td>\n",
              "      <td>1.426052</td>\n",
              "      <td>13.503922</td>\n",
              "      <td>18.493927</td>\n",
              "      <td>4.407097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>105.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>232.000000</td>\n",
              "      <td>108.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-172.000000</td>\n",
              "      <td>-177.000000</td>\n",
              "      <td>-170.000000</td>\n",
              "      <td>-135.000000</td>\n",
              "      <td>-179.000000</td>\n",
              "      <td>44.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>-32.900000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-48.400000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-5.600000</td>\n",
              "      <td>-3.100000</td>\n",
              "      <td>-11.800000</td>\n",
              "      <td>-242.400000</td>\n",
              "      <td>-146.200000</td>\n",
              "      <td>-3.200000</td>\n",
              "      <td>-20.400000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-42.900000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-0.900000</td>\n",
              "      <td>-2.600000</td>\n",
              "      <td>-8.200000</td>\n",
              "      <td>-124.800000</td>\n",
              "      <td>-161.400000</td>\n",
              "      <td>-4.800000</td>\n",
              "      <td>-14.200000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-30.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.900000</td>\n",
              "      <td>-5.000000</td>\n",
              "      <td>-56.800000</td>\n",
              "      <td>-63.600000</td>\n",
              "      <td>-5.600000</td>\n",
              "      <td>-4.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-28.600000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.800000</td>\n",
              "      <td>-6.000000</td>\n",
              "      <td>-44.200000</td>\n",
              "      <td>-38.600000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>36.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>160.000000</td>\n",
              "      <td>59.000000</td>\n",
              "      <td>80.000000</td>\n",
              "      <td>142.000000</td>\n",
              "      <td>350.000000</td>\n",
              "      <td>148.000000</td>\n",
              "      <td>79.000000</td>\n",
              "      <td>3.750000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>-124.500000</td>\n",
              "      <td>65.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>44.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.200000</td>\n",
              "      <td>-13.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>-19.525000</td>\n",
              "      <td>9.850000</td>\n",
              "      <td>-0.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.875000</td>\n",
              "      <td>-9.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>1.100000</td>\n",
              "      <td>-0.925000</td>\n",
              "      <td>11.275000</td>\n",
              "      <td>-0.600000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.100000</td>\n",
              "      <td>-4.725000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>8.675000</td>\n",
              "      <td>15.375000</td>\n",
              "      <td>-0.500000</td>\n",
              "      <td>-0.425000</td>\n",
              "      <td>6.600000</td>\n",
              "      <td>-2.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>11.450000</td>\n",
              "      <td>17.550000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>47.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>164.000000</td>\n",
              "      <td>68.000000</td>\n",
              "      <td>86.000000</td>\n",
              "      <td>157.000000</td>\n",
              "      <td>367.000000</td>\n",
              "      <td>162.000000</td>\n",
              "      <td>91.000000</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>56.000000</td>\n",
              "      <td>40.000000</td>\n",
              "      <td>-50.500000</td>\n",
              "      <td>72.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>48.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>48.000000</td>\n",
              "      <td>20.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.050000</td>\n",
              "      <td>-8.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.300000</td>\n",
              "      <td>3.800000</td>\n",
              "      <td>-4.700000</td>\n",
              "      <td>32.550000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>11.200000</td>\n",
              "      <td>-6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>2.400000</td>\n",
              "      <td>11.400000</td>\n",
              "      <td>32.750000</td>\n",
              "      <td>-0.200000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>-3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>1.750000</td>\n",
              "      <td>18.350000</td>\n",
              "      <td>30.350000</td>\n",
              "      <td>-0.200000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.800000</td>\n",
              "      <td>-1.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.350000</td>\n",
              "      <td>18.100000</td>\n",
              "      <td>27.900000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>58.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>170.000000</td>\n",
              "      <td>79.000000</td>\n",
              "      <td>94.000000</td>\n",
              "      <td>175.000000</td>\n",
              "      <td>384.000000</td>\n",
              "      <td>179.000000</td>\n",
              "      <td>102.000000</td>\n",
              "      <td>66.000000</td>\n",
              "      <td>63.250000</td>\n",
              "      <td>65.000000</td>\n",
              "      <td>62.000000</td>\n",
              "      <td>117.250000</td>\n",
              "      <td>81.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>36.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>64.000000</td>\n",
              "      <td>36.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>36.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>28.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>10.600000</td>\n",
              "      <td>-5.800000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>5.525000</td>\n",
              "      <td>7.625000</td>\n",
              "      <td>56.025000</td>\n",
              "      <td>0.225000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>15.100000</td>\n",
              "      <td>-3.700000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>3.900000</td>\n",
              "      <td>25.050000</td>\n",
              "      <td>52.325000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>14.125000</td>\n",
              "      <td>-1.900000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>2.800000</td>\n",
              "      <td>27.900000</td>\n",
              "      <td>48.100000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>11.200000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>2.100000</td>\n",
              "      <td>25.825000</td>\n",
              "      <td>41.125000</td>\n",
              "      <td>6.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>83.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>780.000000</td>\n",
              "      <td>176.000000</td>\n",
              "      <td>188.000000</td>\n",
              "      <td>524.000000</td>\n",
              "      <td>509.000000</td>\n",
              "      <td>381.000000</td>\n",
              "      <td>205.000000</td>\n",
              "      <td>169.000000</td>\n",
              "      <td>179.000000</td>\n",
              "      <td>176.000000</td>\n",
              "      <td>166.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>163.000000</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>156.000000</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>76.000000</td>\n",
              "      <td>132.000000</td>\n",
              "      <td>92.00000</td>\n",
              "      <td>36.000000</td>\n",
              "      <td>56.000000</td>\n",
              "      <td>76.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>92.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>28.400000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.500000</td>\n",
              "      <td>18.800000</td>\n",
              "      <td>165.400000</td>\n",
              "      <td>137.800000</td>\n",
              "      <td>9.700000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>36.400000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.400000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.800000</td>\n",
              "      <td>15.600000</td>\n",
              "      <td>103.400000</td>\n",
              "      <td>182.300000</td>\n",
              "      <td>3.400000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>29.500000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.800000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.800000</td>\n",
              "      <td>8.300000</td>\n",
              "      <td>82.100000</td>\n",
              "      <td>127.900000</td>\n",
              "      <td>2.700000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>23.600000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.400000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>88.800000</td>\n",
              "      <td>115.900000</td>\n",
              "      <td>16.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 280 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              0           1           2    ...         277         278         279\n",
              "count  452.000000  452.000000  452.000000  ...  452.000000  452.000000  452.000000\n",
              "mean    46.471239    0.550885  166.188053  ...   19.326106   29.473230    3.880531\n",
              "std     16.466631    0.497955   37.170340  ...   13.503922   18.493927    4.407097\n",
              "min      0.000000    0.000000  105.000000  ...  -44.200000  -38.600000    1.000000\n",
              "25%     36.000000    0.000000  160.000000  ...   11.450000   17.550000    1.000000\n",
              "50%     47.000000    1.000000  164.000000  ...   18.100000   27.900000    1.000000\n",
              "75%     58.000000    1.000000  170.000000  ...   25.825000   41.125000    6.000000\n",
              "max     83.000000    1.000000  780.000000  ...   88.800000  115.900000   16.000000\n",
              "\n",
              "[8 rows x 280 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3G4RkFsDisRs",
        "outputId": "170eb64f-5ded-4a6a-93e2-910f53e7926f"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 438 entries, 1 to 451\n",
            "Columns: 279 entries, Age to Target\n",
            "dtypes: float64(124), int64(155)\n",
            "memory usage: 958.1 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "qr6IqQU0Kpw2",
        "outputId": "2f3624cc-030c-4ce5-a139-a8736bcc53f8"
      },
      "source": [
        "column_indices = [0,1,2,3,4,5,6,7,8,9,10,11,12,14,279]\n",
        "new_names = [\n",
        "             \"Age\", \"Sex\", \"Height\", \"Weight\",\n",
        "             \"QRS duration\",\"P-R interval\",\n",
        "             \"Q-T interval\",\"T interval\",\n",
        "             \"P interval\",\"QRS\",\"T\",\"P\",\n",
        "             \"QRST\",\"Heart rate\",\"Target\"\n",
        "]\n",
        "old_names = df.columns[column_indices]\n",
        "df.rename(columns=dict(zip(old_names, new_names)), inplace=True)\n",
        "df.head(5)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Height</th>\n",
              "      <th>Weight</th>\n",
              "      <th>QRS duration</th>\n",
              "      <th>P-R interval</th>\n",
              "      <th>Q-T interval</th>\n",
              "      <th>T interval</th>\n",
              "      <th>P interval</th>\n",
              "      <th>QRS</th>\n",
              "      <th>T</th>\n",
              "      <th>P</th>\n",
              "      <th>QRST</th>\n",
              "      <th>13</th>\n",
              "      <th>Heart rate</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>240</th>\n",
              "      <th>241</th>\n",
              "      <th>242</th>\n",
              "      <th>243</th>\n",
              "      <th>244</th>\n",
              "      <th>245</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "      <th>259</th>\n",
              "      <th>260</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>75</td>\n",
              "      <td>0</td>\n",
              "      <td>190</td>\n",
              "      <td>80</td>\n",
              "      <td>91</td>\n",
              "      <td>193</td>\n",
              "      <td>371</td>\n",
              "      <td>174</td>\n",
              "      <td>121</td>\n",
              "      <td>-16</td>\n",
              "      <td>13.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>63.0</td>\n",
              "      <td>0</td>\n",
              "      <td>52</td>\n",
              "      <td>44</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>44</td>\n",
              "      <td>20</td>\n",
              "      <td>36</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>52</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.4</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>5.9</td>\n",
              "      <td>-3.9</td>\n",
              "      <td>52.7</td>\n",
              "      <td>-0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>-8.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>5.1</td>\n",
              "      <td>17.7</td>\n",
              "      <td>70.7</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.5</td>\n",
              "      <td>-4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>3.9</td>\n",
              "      <td>25.5</td>\n",
              "      <td>62.9</td>\n",
              "      <td>-0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>-0.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>2.9</td>\n",
              "      <td>23.3</td>\n",
              "      <td>49.4</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>165</td>\n",
              "      <td>64</td>\n",
              "      <td>81</td>\n",
              "      <td>174</td>\n",
              "      <td>401</td>\n",
              "      <td>149</td>\n",
              "      <td>39</td>\n",
              "      <td>25</td>\n",
              "      <td>37.0</td>\n",
              "      <td>-17.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>53.0</td>\n",
              "      <td>0</td>\n",
              "      <td>48</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.8</td>\n",
              "      <td>-7.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>3.8</td>\n",
              "      <td>-5.7</td>\n",
              "      <td>27.7</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.5</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2.6</td>\n",
              "      <td>11.8</td>\n",
              "      <td>34.6</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>-2.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>2.6</td>\n",
              "      <td>21.6</td>\n",
              "      <td>43.4</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>2.1</td>\n",
              "      <td>20.4</td>\n",
              "      <td>38.8</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>54</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>95</td>\n",
              "      <td>138</td>\n",
              "      <td>163</td>\n",
              "      <td>386</td>\n",
              "      <td>185</td>\n",
              "      <td>102</td>\n",
              "      <td>96</td>\n",
              "      <td>34.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>80</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>56</td>\n",
              "      <td>52</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.8</td>\n",
              "      <td>-4.1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.3</td>\n",
              "      <td>20.4</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-5.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2.2</td>\n",
              "      <td>-3.0</td>\n",
              "      <td>20.7</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.1</td>\n",
              "      <td>-3.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>3.4</td>\n",
              "      <td>11.5</td>\n",
              "      <td>48.2</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.5</td>\n",
              "      <td>-2.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>3.4</td>\n",
              "      <td>12.3</td>\n",
              "      <td>49.0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>55</td>\n",
              "      <td>0</td>\n",
              "      <td>175</td>\n",
              "      <td>94</td>\n",
              "      <td>100</td>\n",
              "      <td>202</td>\n",
              "      <td>380</td>\n",
              "      <td>179</td>\n",
              "      <td>143</td>\n",
              "      <td>28</td>\n",
              "      <td>11.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>71.0</td>\n",
              "      <td>0</td>\n",
              "      <td>72</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>48</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>64</td>\n",
              "      <td>36</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>36</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>-7.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>4.1</td>\n",
              "      <td>7.6</td>\n",
              "      <td>51.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>-5.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>3.3</td>\n",
              "      <td>28.8</td>\n",
              "      <td>63.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>-3.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>3.0</td>\n",
              "      <td>36.8</td>\n",
              "      <td>68.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.2</td>\n",
              "      <td>-2.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>2.6</td>\n",
              "      <td>34.6</td>\n",
              "      <td>61.6</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>75</td>\n",
              "      <td>0</td>\n",
              "      <td>190</td>\n",
              "      <td>80</td>\n",
              "      <td>88</td>\n",
              "      <td>181</td>\n",
              "      <td>360</td>\n",
              "      <td>177</td>\n",
              "      <td>103</td>\n",
              "      <td>-16</td>\n",
              "      <td>13.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>48</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>52</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.5</td>\n",
              "      <td>-10.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>4.7</td>\n",
              "      <td>-4.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>-7.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>4.9</td>\n",
              "      <td>16.2</td>\n",
              "      <td>63.2</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.1</td>\n",
              "      <td>-0.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>2.9</td>\n",
              "      <td>21.7</td>\n",
              "      <td>48.9</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.1</td>\n",
              "      <td>-3.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>3.9</td>\n",
              "      <td>25.4</td>\n",
              "      <td>62.8</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 280 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Age  Sex  Height  Weight  QRS duration  ...  275  276   277   278  Target\n",
              "0   75    0     190      80            91  ...  0.9  2.9  23.3  49.4       8\n",
              "1   56    1     165      64            81  ...  0.2  2.1  20.4  38.8       6\n",
              "2   54    0     172      95           138  ...  0.3  3.4  12.3  49.0      10\n",
              "3   55    0     175      94           100  ...  0.4  2.6  34.6  61.6       1\n",
              "4   75    0     190      80            88  ... -0.1  3.9  25.4  62.8       7\n",
              "\n",
              "[5 rows x 280 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZPJtoO8K9px",
        "outputId": "374abee2-cf7d-450f-8945-9f45d8944efa"
      },
      "source": [
        "corr_matrix = df.corr()\n",
        "corr_pairs = corr_matrix.unstack()\n",
        "sorted_pairs = corr_pairs.sort_values(kind=\"quicksort\")\n",
        "strong_pairs = sorted_pairs[abs(sorted_pairs) > 0.5]\n",
        "strong_pairs = strong_pairs[abs(strong_pairs) < 1]\n",
        "\n",
        "print(strong_pairs)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "194  55    -1.000000\n",
            "55   194   -1.000000\n",
            "254  127   -0.999287\n",
            "127  254   -0.999287\n",
            "214  79    -0.967353\n",
            "              ...   \n",
            "203  66     0.941517\n",
            "115  243    0.966664\n",
            "243  115    0.966664\n",
            "150  273    0.997769\n",
            "273  150    0.997769\n",
            "Length: 1292, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdq9CQt8aI28",
        "outputId": "feefd532-6293-403d-d1a8-d0f4af9dc8b5"
      },
      "source": [
        "corr_matrix['Target'].sort_values(ascending=False).head(20)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Target          1.000000\n",
              "90              0.368876\n",
              "QRS duration    0.323879\n",
              "92              0.313982\n",
              "102             0.282523\n",
              "223             0.235488\n",
              "233             0.218811\n",
              "17              0.195198\n",
              "29              0.183083\n",
              "94              0.174346\n",
              "52              0.173243\n",
              "125             0.170670\n",
              "191             0.165693\n",
              "68              0.152534\n",
              "239             0.151782\n",
              "77              0.143284\n",
              "152             0.141506\n",
              "221             0.141274\n",
              "56              0.141103\n",
              "113             0.140502\n",
              "Name: Target, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJj20t7Ipghc"
      },
      "source": [
        "## 3. Wstepne przetwarzanie "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "kqTSdsVvJp7z",
        "outputId": "5e29717e-c0ff-467f-aa64-0fdbc1df3205"
      },
      "source": [
        "miss_total = df.isnull().sum().sort_values(ascending=False)\n",
        "miss_relative=(df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
        "miss_data = pd.concat([miss_total, miss_relative], axis=1, keys=['Total', 'Relative'])\n",
        "miss_data.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Total</th>\n",
              "      <th>Relative</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>376</td>\n",
              "      <td>0.831858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>P</th>\n",
              "      <td>22</td>\n",
              "      <td>0.048673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>T</th>\n",
              "      <td>8</td>\n",
              "      <td>0.017699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Heart rate</th>\n",
              "      <td>1</td>\n",
              "      <td>0.002212</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>QRST</th>\n",
              "      <td>1</td>\n",
              "      <td>0.002212</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Total  Relative\n",
              "13            376  0.831858\n",
              "P              22  0.048673\n",
              "T               8  0.017699\n",
              "Heart rate      1  0.002212\n",
              "QRST            1  0.002212"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OusqKC1qJ3uO"
      },
      "source": [
        "df = df.drop(columns=[13])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "Ov2fKu8RWjF2",
        "outputId": "c3295c95-f0fc-42e1-f975-5d99e43b8c14"
      },
      "source": [
        "df = df.apply(pd.to_numeric, errors='coerce')\n",
        "df.fillna(df.mean(), inplace=True)\n",
        "df.head(5)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Height</th>\n",
              "      <th>Weight</th>\n",
              "      <th>QRS duration</th>\n",
              "      <th>P-R interval</th>\n",
              "      <th>Q-T interval</th>\n",
              "      <th>T interval</th>\n",
              "      <th>P interval</th>\n",
              "      <th>QRS</th>\n",
              "      <th>T</th>\n",
              "      <th>P</th>\n",
              "      <th>QRST</th>\n",
              "      <th>Heart rate</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>...</th>\n",
              "      <th>240</th>\n",
              "      <th>241</th>\n",
              "      <th>242</th>\n",
              "      <th>243</th>\n",
              "      <th>244</th>\n",
              "      <th>245</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "      <th>259</th>\n",
              "      <th>260</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>75</td>\n",
              "      <td>0</td>\n",
              "      <td>190</td>\n",
              "      <td>80</td>\n",
              "      <td>91</td>\n",
              "      <td>193</td>\n",
              "      <td>371</td>\n",
              "      <td>174</td>\n",
              "      <td>121</td>\n",
              "      <td>-16</td>\n",
              "      <td>13.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>-2.0</td>\n",
              "      <td>63.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>52</td>\n",
              "      <td>44</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>44</td>\n",
              "      <td>20</td>\n",
              "      <td>36</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>52</td>\n",
              "      <td>40</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.4</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>5.9</td>\n",
              "      <td>-3.9</td>\n",
              "      <td>52.7</td>\n",
              "      <td>-0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>-8.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>5.1</td>\n",
              "      <td>17.7</td>\n",
              "      <td>70.7</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.5</td>\n",
              "      <td>-4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>3.9</td>\n",
              "      <td>25.5</td>\n",
              "      <td>62.9</td>\n",
              "      <td>-0.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>-0.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>2.9</td>\n",
              "      <td>23.3</td>\n",
              "      <td>49.4</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>56</td>\n",
              "      <td>1</td>\n",
              "      <td>165</td>\n",
              "      <td>64</td>\n",
              "      <td>81</td>\n",
              "      <td>174</td>\n",
              "      <td>401</td>\n",
              "      <td>149</td>\n",
              "      <td>39</td>\n",
              "      <td>25</td>\n",
              "      <td>37.0</td>\n",
              "      <td>-17.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>53.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>48</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>64</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>24</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.8</td>\n",
              "      <td>-7.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.9</td>\n",
              "      <td>3.8</td>\n",
              "      <td>-5.7</td>\n",
              "      <td>27.7</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.5</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2.6</td>\n",
              "      <td>11.8</td>\n",
              "      <td>34.6</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>-2.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>2.6</td>\n",
              "      <td>21.6</td>\n",
              "      <td>43.4</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.2</td>\n",
              "      <td>2.1</td>\n",
              "      <td>20.4</td>\n",
              "      <td>38.8</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>54</td>\n",
              "      <td>0</td>\n",
              "      <td>172</td>\n",
              "      <td>95</td>\n",
              "      <td>138</td>\n",
              "      <td>163</td>\n",
              "      <td>386</td>\n",
              "      <td>185</td>\n",
              "      <td>102</td>\n",
              "      <td>96</td>\n",
              "      <td>34.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>66.0</td>\n",
              "      <td>75.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>80</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>56</td>\n",
              "      <td>52</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>116</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.8</td>\n",
              "      <td>-4.1</td>\n",
              "      <td>4.0</td>\n",
              "      <td>-0.5</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.3</td>\n",
              "      <td>20.4</td>\n",
              "      <td>23.3</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-5.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>2.2</td>\n",
              "      <td>-3.0</td>\n",
              "      <td>20.7</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.1</td>\n",
              "      <td>-3.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>3.4</td>\n",
              "      <td>11.5</td>\n",
              "      <td>48.2</td>\n",
              "      <td>0.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.5</td>\n",
              "      <td>-2.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.3</td>\n",
              "      <td>3.4</td>\n",
              "      <td>12.3</td>\n",
              "      <td>49.0</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>55</td>\n",
              "      <td>0</td>\n",
              "      <td>175</td>\n",
              "      <td>94</td>\n",
              "      <td>100</td>\n",
              "      <td>202</td>\n",
              "      <td>380</td>\n",
              "      <td>179</td>\n",
              "      <td>143</td>\n",
              "      <td>28</td>\n",
              "      <td>11.0</td>\n",
              "      <td>-5.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>72</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>48</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>64</td>\n",
              "      <td>36</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>36</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>52</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>-7.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>4.1</td>\n",
              "      <td>7.6</td>\n",
              "      <td>51.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>-5.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>3.3</td>\n",
              "      <td>28.8</td>\n",
              "      <td>63.1</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>-3.7</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.6</td>\n",
              "      <td>3.0</td>\n",
              "      <td>36.8</td>\n",
              "      <td>68.0</td>\n",
              "      <td>0.1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12.2</td>\n",
              "      <td>-2.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4</td>\n",
              "      <td>2.6</td>\n",
              "      <td>34.6</td>\n",
              "      <td>61.6</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>75</td>\n",
              "      <td>0</td>\n",
              "      <td>190</td>\n",
              "      <td>80</td>\n",
              "      <td>88</td>\n",
              "      <td>181</td>\n",
              "      <td>360</td>\n",
              "      <td>177</td>\n",
              "      <td>103</td>\n",
              "      <td>-16</td>\n",
              "      <td>13.0</td>\n",
              "      <td>61.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>74.463415</td>\n",
              "      <td>0</td>\n",
              "      <td>48</td>\n",
              "      <td>40</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>40</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>24</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>52</td>\n",
              "      <td>36</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.5</td>\n",
              "      <td>-10.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>4.7</td>\n",
              "      <td>-4.0</td>\n",
              "      <td>43.0</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>15.2</td>\n",
              "      <td>-7.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>4.9</td>\n",
              "      <td>16.2</td>\n",
              "      <td>63.2</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.1</td>\n",
              "      <td>-0.9</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.2</td>\n",
              "      <td>2.9</td>\n",
              "      <td>21.7</td>\n",
              "      <td>48.9</td>\n",
              "      <td>-0.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13.1</td>\n",
              "      <td>-3.6</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.1</td>\n",
              "      <td>3.9</td>\n",
              "      <td>25.4</td>\n",
              "      <td>62.8</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 279 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Age  Sex  Height  Weight  QRS duration  ...  275  276   277   278  Target\n",
              "0   75    0     190      80            91  ...  0.9  2.9  23.3  49.4       8\n",
              "1   56    1     165      64            81  ...  0.2  2.1  20.4  38.8       6\n",
              "2   54    0     172      95           138  ...  0.3  3.4  12.3  49.0      10\n",
              "3   55    0     175      94           100  ...  0.4  2.6  34.6  61.6       1\n",
              "4   75    0     190      80            88  ... -0.1  3.9  25.4  62.8       7\n",
              "\n",
              "[5 rows x 279 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "QGQP21Y_oV7n",
        "outputId": "61c1bece-c6f0-4162-fefb-b50f9477adf3"
      },
      "source": [
        "df.groupby('Target').count().sort_values([278],ascending=False)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Age</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Height</th>\n",
              "      <th>Weight</th>\n",
              "      <th>QRS duration</th>\n",
              "      <th>P-R interval</th>\n",
              "      <th>Q-T interval</th>\n",
              "      <th>T interval</th>\n",
              "      <th>P interval</th>\n",
              "      <th>QRS</th>\n",
              "      <th>T</th>\n",
              "      <th>P</th>\n",
              "      <th>QRST</th>\n",
              "      <th>Heart rate</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>...</th>\n",
              "      <th>239</th>\n",
              "      <th>240</th>\n",
              "      <th>241</th>\n",
              "      <th>242</th>\n",
              "      <th>243</th>\n",
              "      <th>244</th>\n",
              "      <th>245</th>\n",
              "      <th>246</th>\n",
              "      <th>247</th>\n",
              "      <th>248</th>\n",
              "      <th>249</th>\n",
              "      <th>250</th>\n",
              "      <th>251</th>\n",
              "      <th>252</th>\n",
              "      <th>253</th>\n",
              "      <th>254</th>\n",
              "      <th>255</th>\n",
              "      <th>256</th>\n",
              "      <th>257</th>\n",
              "      <th>258</th>\n",
              "      <th>259</th>\n",
              "      <th>260</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Target</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>...</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "      <td>245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>...</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>...</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>...</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "      <td>25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>...</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "      <td>22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>...</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>...</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>...</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>...</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>...</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13 rows × 278 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Age  Sex  Height  Weight  QRS duration  ...  274  275  276  277  278\n",
              "Target                                          ...                         \n",
              "1       245  245     245     245           245  ...  245  245  245  245  245\n",
              "10       50   50      50      50            50  ...   50   50   50   50   50\n",
              "2        44   44      44      44            44  ...   44   44   44   44   44\n",
              "6        25   25      25      25            25  ...   25   25   25   25   25\n",
              "16       22   22      22      22            22  ...   22   22   22   22   22\n",
              "3        15   15      15      15            15  ...   15   15   15   15   15\n",
              "4        15   15      15      15            15  ...   15   15   15   15   15\n",
              "5        13   13      13      13            13  ...   13   13   13   13   13\n",
              "9         9    9       9       9             9  ...    9    9    9    9    9\n",
              "15        5    5       5       5             5  ...    5    5    5    5    5\n",
              "14        4    4       4       4             4  ...    4    4    4    4    4\n",
              "7         3    3       3       3             3  ...    3    3    3    3    3\n",
              "8         2    2       2       2             2  ...    2    2    2    2    2\n",
              "\n",
              "[13 rows x 278 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzclR-yqoadH"
      },
      "source": [
        "df = df[(df.Target != 8) & (df.Target != 7) & (df.Target != 14) & (df.Target != 15)]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRKti8X4oqYV",
        "outputId": "986718d0-f1ae-4016-8aff-3b412490e89a"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(438, 279)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFy05yIWWxsg"
      },
      "source": [
        "X = df.iloc[:, :-1]\n",
        "y = df.iloc[:,-1]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=10)\n",
        "\n",
        "scaler=MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test=scaler.transform(X_test)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPKLew5ypt7f"
      },
      "source": [
        "## 4. Klasyfikacja"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrCPtGLrpyT6"
      },
      "source": [
        "### 4.1 KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-IkfuEyqLUL",
        "outputId": "3f7b7a69-1926-4c3b-90ce-fedcbe665dc1"
      },
      "source": [
        "param_grid={'weights':['distance', 'uniform'], 'n_neighbors':range(1,100)}\n",
        "\n",
        "grid_search = GridSearchCV( KNeighborsClassifier(),param_grid, cv = 10)\n",
        "grid_search.fit(X_train, y_train)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 7 members, which is less than n_splits=10.\n",
            "  % (min_groups, self.n_splits)), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=10, error_score=nan,\n",
              "             estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
              "                                            metric='minkowski',\n",
              "                                            metric_params=None, n_jobs=None,\n",
              "                                            n_neighbors=5, p=2,\n",
              "                                            weights='uniform'),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'n_neighbors': range(1, 100),\n",
              "                         'weights': ['distance', 'uniform']},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_VFr3k1vR3I",
        "outputId": "b63fbe98-7c76-46a7-c052-14cc3afb4b4c"
      },
      "source": [
        "print('Best config{}'.format(grid_search.best_estimator_))\n",
        "print('Best score {:.3f}'.format(grid_search.best_score_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best configKNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
            "                     metric_params=None, n_jobs=None, n_neighbors=7, p=2,\n",
            "                     weights='distance')\n",
            "Best score 0.610\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qB00xPYavYcJ",
        "outputId": "f43e81bc-d1e0-4f24-f186-e80aa0f21b02"
      },
      "source": [
        "knn_clf = KNeighborsClassifier(\n",
        "    algorithm='auto', leaf_size=30, metric='minkowski',\n",
        "    metric_params=None, n_jobs=None, n_neighbors=7, p=2,\n",
        "    weights='distance'\n",
        ")\n",
        "knn_clf.fit(X_train,y_train)\n",
        "pred = knn_clf.predict(X_test)\n",
        "print(metrics.classification_report(y_test,pred))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.63      0.98      0.77        63\n",
            "           2       0.00      0.00      0.00        11\n",
            "           3       1.00      0.67      0.80         6\n",
            "           4       1.00      1.00      1.00         1\n",
            "           5       1.00      0.50      0.67         2\n",
            "           6       0.33      0.08      0.13        12\n",
            "           9       0.00      0.00      0.00         2\n",
            "          10       1.00      0.11      0.20         9\n",
            "          16       0.00      0.00      0.00         4\n",
            "\n",
            "    accuracy                           0.64       110\n",
            "   macro avg       0.55      0.37      0.40       110\n",
            "weighted avg       0.56      0.64      0.53       110\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMKqo742JgLI",
        "outputId": "d3a624c9-263e-4d42-a3ad-e8ff42c8078d"
      },
      "source": [
        "print(metrics.f1_score(y_test, pred, average='weighted'))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5341414141414141\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSar6LMiqDst"
      },
      "source": [
        "### 4.2 Regresja logistyczna"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkpgaXQPpAFK",
        "outputId": "71f24c50-af05-4384-c3ca-78acca188b3f"
      },
      "source": [
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
        "\n",
        "grid_search_log = GridSearchCV(LogisticRegression(penalty='l2'), param_grid, cv=5)\n",
        "grid_search_log.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                          fit_intercept=True,\n",
              "                                          intercept_scaling=1, l1_ratio=None,\n",
              "                                          max_iter=100, multi_class='auto',\n",
              "                                          n_jobs=None, penalty='l2',\n",
              "                                          random_state=None, solver='lbfgs',\n",
              "                                          tol=0.0001, verbose=0,\n",
              "                                          warm_start=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q96Rfrf-3xDO",
        "outputId": "d70cff17-3f40-4ebd-d2a3-57b1e38537bd"
      },
      "source": [
        "print('Best config{}'.format(grid_search_log.best_estimator_))\n",
        "print('Best score {:.3f}'.format(grid_search_log.best_score_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best configLogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Best score 0.719\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxgtlMGf3xyx",
        "outputId": "a06f00e2-7a37-4b28-f97c-e4a55135db95"
      },
      "source": [
        "log = LogisticRegression(\n",
        "    C=1, class_weight=None, dual=False, fit_intercept=True,\n",
        "    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
        "    multi_class='auto', n_jobs=None, penalty='l2',\n",
        "    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
        "    warm_start=False\n",
        ")\n",
        "log.fit(X_train, y_train)\n",
        "pred = log.predict(X_test)\n",
        "print(metrics.classification_report(y_test,pred))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.74      1.00      0.85        63\n",
            "           2       0.71      0.45      0.56        11\n",
            "           3       0.83      0.83      0.83         6\n",
            "           4       0.33      1.00      0.50         1\n",
            "           5       1.00      0.50      0.67         2\n",
            "           6       1.00      0.08      0.15        12\n",
            "           9       0.00      0.00      0.00         2\n",
            "          10       0.86      0.67      0.75         9\n",
            "          16       0.00      0.00      0.00         4\n",
            "\n",
            "    accuracy                           0.75       110\n",
            "   macro avg       0.61      0.50      0.48       110\n",
            "weighted avg       0.74      0.75      0.68       110\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVd1FOxyJ3bv",
        "outputId": "aedbc58d-db33-41b6-9d8e-1fac7461bd42"
      },
      "source": [
        "print(metrics.f1_score(y_test, pred, average='weighted'))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6834157584157584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Khe6y2lqoeW"
      },
      "source": [
        "### 4.3 Las losowy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajg2JiVSqrwd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10101b0f-0c5f-4d17-9f68-0348f5ee6a03"
      },
      "source": [
        "param_grid = {\"max_depth\": range(1,20),\n",
        "              \"max_features\": sp_randint(1, 40),\n",
        "              \"min_samples_split\": sp_randint(2, 30),\n",
        "              \"min_samples_leaf\": sp_randint(1, 20),\n",
        "              \"bootstrap\": [True, False]}\n",
        "grid_search_RF = RandomizedSearchCV(RandomForestClassifier(n_estimators=1000), param_distributions=param_grid,\n",
        "                                   n_iter=30, random_state=0,n_jobs=-1)\n",
        "grid_search_RF.fit(X_train, y_train)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=None, error_score=nan,\n",
              "                   estimator=RandomForestClassifier(bootstrap=True,\n",
              "                                                    ccp_alpha=0.0,\n",
              "                                                    class_weight=None,\n",
              "                                                    criterion='gini',\n",
              "                                                    max_depth=None,\n",
              "                                                    max_features='auto',\n",
              "                                                    max_leaf_nodes=None,\n",
              "                                                    max_samples=None,\n",
              "                                                    min_impurity_decrease=0.0,\n",
              "                                                    min_impurity_split=None,\n",
              "                                                    min_samples_leaf=1,\n",
              "                                                    min_samples_split=2,\n",
              "                                                    min_weight_fraction_leaf=0.0,\n",
              "                                                    n_estimators=1000,\n",
              "                                                    n_...\n",
              "                                        'max_features': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fb4f6aa3ad0>,\n",
              "                                        'min_samples_leaf': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fb4f6aeeed0>,\n",
              "                                        'min_samples_split': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fb4f6aa3d10>},\n",
              "                   pre_dispatch='2*n_jobs', random_state=0, refit=True,\n",
              "                   return_train_score=False, scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVK5uUzwAJXl",
        "outputId": "96fbc703-43d1-4f8d-d886-77778112a271"
      },
      "source": [
        "print('Best config{}'.format(grid_search_RF.best_estimator_))\n",
        "print('Best score {:.3f}'.format(grid_search_RF.best_score_))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best configRandomForestClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
            "                       criterion='gini', max_depth=15, max_features=33,\n",
            "                       max_leaf_nodes=None, max_samples=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=2, min_samples_split=11,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
            "                       n_jobs=None, oob_score=False, random_state=None,\n",
            "                       verbose=0, warm_start=False)\n",
            "Best score 0.762\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HULqAuhIAMBj",
        "outputId": "d50d80e9-e732-431d-9f78-905900463f9a"
      },
      "source": [
        "rf_clf = RandomForestClassifier(\n",
        "    bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
        "    criterion='gini', max_depth=15, max_features=33,\n",
        "    max_leaf_nodes=None, max_samples=None,\n",
        "    min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "    min_samples_leaf=2, min_samples_split=11,\n",
        "    min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
        "    n_jobs=None, oob_score=False, random_state=None,\n",
        "    verbose=0, warm_start=False).fit(X_train,y_train)\n",
        "\n",
        "pred = rf_clf.predict(X_test)    \n",
        "print(metrics.classification_report(y_test,pred))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.85      0.97      0.90        63\n",
            "           2       0.86      0.55      0.67        11\n",
            "           3       0.71      0.83      0.77         6\n",
            "           4       0.33      1.00      0.50         1\n",
            "           5       0.00      0.00      0.00         2\n",
            "           6       0.80      0.67      0.73        12\n",
            "           9       1.00      1.00      1.00         2\n",
            "          10       0.78      0.78      0.78         9\n",
            "          16       0.00      0.00      0.00         4\n",
            "\n",
            "    accuracy                           0.82       110\n",
            "   macro avg       0.59      0.64      0.59       110\n",
            "weighted avg       0.78      0.82      0.79       110\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UaIzWRTBJ7Az",
        "outputId": "2608b94b-4709-4daf-ab4e-9c37ce9e0fe1"
      },
      "source": [
        "print(metrics.f1_score(y_test, pred, average='weighted'))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7919029455393092\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6GKqf_9qwOi"
      },
      "source": [
        "### 4.4 SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riSI2oxkq20g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd7027e2-8327-47ed-9c23-6c2079d5c5f2"
      },
      "source": [
        "svc = SVC()\n",
        "param_grid = {'kernel':('rbf', 'sigmoid','linear'),\n",
        "              'C':[0.001, 0.01, 0.1, 0.5, 1, 10, 50, 100, 1000],\n",
        "              'gamma':[0.001, 0.01, 0.1, 0.5, 1, 10]}\n",
        "\n",
        "grid_search_svc = GridSearchCV(svc, param_grid, cv = 5)\n",
        "grid_search_svc.fit(X_train, y_train)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
              "                           class_weight=None, coef0=0.0,\n",
              "                           decision_function_shape='ovr', degree=3,\n",
              "                           gamma='scale', kernel='rbf', max_iter=-1,\n",
              "                           probability=False, random_state=None, shrinking=True,\n",
              "                           tol=0.001, verbose=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'C': [0.001, 0.01, 0.1, 0.5, 1, 10, 50, 100, 1000],\n",
              "                         'gamma': [0.001, 0.01, 0.1, 0.5, 1, 10],\n",
              "                         'kernel': ('rbf', 'sigmoid', 'linear')},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aY2pqGMMArKz",
        "outputId": "7551ca13-b518-475a-e647-63861f8c0982"
      },
      "source": [
        "print('Best config{}'.format(grid_search_svc.best_estimator_))\n",
        "print('Best score {:.3f}'.format(grid_search_svc.best_score_))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best configSVC(C=0.5, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='linear',\n",
            "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
            "    tol=0.001, verbose=False)\n",
            "Best score 0.741\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_AocBZGAyDB",
        "outputId": "c5279d76-eb7a-4eb8-b27d-55218a55628c"
      },
      "source": [
        "svc_clf = SVC(\n",
        "    C=0.5, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
        "    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='linear',\n",
        "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
        "    tol=0.001, verbose=False).fit(X_train, y_train)\n",
        "    \n",
        "pred = svc_clf.predict(X_test)    \n",
        "print(metrics.classification_report(y_test,pred))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.72      1.00      0.83        63\n",
            "           2       1.00      0.27      0.43        11\n",
            "           3       0.71      0.83      0.77         6\n",
            "           4       0.33      1.00      0.50         1\n",
            "           5       1.00      0.50      0.67         2\n",
            "           6       0.00      0.00      0.00        12\n",
            "           9       1.00      1.00      1.00         2\n",
            "          10       1.00      0.67      0.80         9\n",
            "          16       0.00      0.00      0.00         4\n",
            "\n",
            "    accuracy                           0.74       110\n",
            "   macro avg       0.64      0.59      0.56       110\n",
            "weighted avg       0.67      0.74      0.66       110\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ie44mHYOJ_4O",
        "outputId": "f26eae67-850a-4822-ed83-bedd91b5209c"
      },
      "source": [
        "print(metrics.f1_score(y_test, pred, average='weighted'))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6630230916985884\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBITXnGnq3eH"
      },
      "source": [
        "### 4.5 Liniowy SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CX_Bi01Nq6yb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "947b4cb0-9be6-4f6d-a305-562e06a74b63"
      },
      "source": [
        "lin = LinearSVC()\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 0.5, 1, 10, 50, 100, 1000],\n",
        "    'max_iter':[1000,10000]}\n",
        "\n",
        "grid_search_lsvc = GridSearchCV(lin, param_grid, cv = 5)\n",
        "grid_search_lsvc.fit(X_train, y_train)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=LinearSVC(C=1.0, class_weight=None, dual=True,\n",
              "                                 fit_intercept=True, intercept_scaling=1,\n",
              "                                 loss='squared_hinge', max_iter=1000,\n",
              "                                 multi_class='ovr', penalty='l2',\n",
              "                                 random_state=None, tol=0.0001, verbose=0),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'C': [0.001, 0.01, 0.1, 0.5, 1, 10, 50, 100, 1000],\n",
              "                         'max_iter': [1000, 10000]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnUtoSYVBnTN",
        "outputId": "85b9c516-debc-434b-8dbd-293f166fc9f8"
      },
      "source": [
        "print('Best config{}'.format(grid_search_lsvc.best_estimator_))\n",
        "print('Best score {:.3f}'.format(grid_search_lsvc.best_score_))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best configLinearSVC(C=0.5, class_weight=None, dual=True, fit_intercept=True,\n",
            "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
            "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
            "          verbose=0)\n",
            "Best score 0.716\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmJWD89TBo7n",
        "outputId": "022dcba9-2edf-425a-b4b8-ebc0362bb46d"
      },
      "source": [
        "lin_clf = LinearSVC(\n",
        "    C=0.5, class_weight=None, dual=True, fit_intercept=True,\n",
        "    intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
        "    multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
        "    verbose=0).fit(X_train, y_train)\n",
        "    \n",
        "pred = lin_clf.predict(X_test)    \n",
        "print(metrics.classification_report(y_test,pred))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.78      0.98      0.87        63\n",
            "           2       0.67      0.55      0.60        11\n",
            "           3       0.83      0.83      0.83         6\n",
            "           4       0.50      1.00      0.67         1\n",
            "           5       1.00      0.50      0.67         2\n",
            "           6       1.00      0.17      0.29        12\n",
            "           9       1.00      0.50      0.67         2\n",
            "          10       0.67      0.67      0.67         9\n",
            "          16       0.00      0.00      0.00         4\n",
            "\n",
            "    accuracy                           0.76       110\n",
            "   macro avg       0.72      0.58      0.58       110\n",
            "weighted avg       0.76      0.76      0.72       110\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDOMCJrTGkdn",
        "outputId": "a116209e-780d-43a3-b8ac-0a3b3f400000"
      },
      "source": [
        "print(metrics.f1_score(y_test, pred, average='weighted'))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7181025035570491\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jSM9thgKFL5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruuBlIwWLzSA"
      },
      "source": [
        "## 5. Klasyfikacja po redukcji danych (Te same nastawy algorytmów)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joLnpCsSKsEO"
      },
      "source": [
        "pca = PCA().fit(X_train)\n",
        "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
        "d = np.argmax(cumsum >= 0.95) + 1"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUeTG2NeKtC7",
        "outputId": "810db9bb-4959-4d14-8d21-83a006ba0360"
      },
      "source": [
        "d"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "DyjZ1tD1KuZ-",
        "outputId": "b02a1c58-7da9-48b5-932f-4fbe86484ead"
      },
      "source": [
        "print('Cumulative sum of Variance')\n",
        "plt.plot(cumsum)\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance');"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cumulative sum of Variance\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RA1LX79GK4yp"
      },
      "source": [
        "pca = PCA(n_components = 80)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKLPZ6OJMHXK"
      },
      "source": [
        "### 5.1 KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4L5PD3X-LAct",
        "outputId": "692898c4-1875-41a8-d811-913ebcb60230"
      },
      "source": [
        "knn_clf = KNeighborsClassifier(\n",
        "    algorithm='auto', leaf_size=30, metric='minkowski',\n",
        "    metric_params=None, n_jobs=None, n_neighbors=7, p=2,\n",
        "    weights='distance'\n",
        ")\n",
        "knn_clf.fit(X_train_pca,y_train)\n",
        "pred = knn_clf.predict(X_test_pca)\n",
        "print(metrics.classification_report(y_test,pred))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.62      0.98      0.76        63\n",
            "           2       0.00      0.00      0.00        11\n",
            "           3       1.00      0.67      0.80         6\n",
            "           4       1.00      1.00      1.00         1\n",
            "           5       1.00      0.50      0.67         2\n",
            "           6       0.50      0.08      0.14        12\n",
            "           9       0.00      0.00      0.00         2\n",
            "          10       1.00      0.22      0.36         9\n",
            "          16       0.00      0.00      0.00         4\n",
            "\n",
            "    accuracy                           0.65       110\n",
            "   macro avg       0.57      0.38      0.41       110\n",
            "weighted avg       0.57      0.65      0.55       110\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIhACCujLXFJ",
        "outputId": "72768988-2214-4c74-bd44-87b701b10756"
      },
      "source": [
        "print(metrics.f1_score(y_test, pred, average='weighted'))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5458793335313135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDP3wlP1MMB7"
      },
      "source": [
        "### 5.2 Regresja logistyczna"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bB8shoGqLZSp",
        "outputId": "bf35414d-a452-4e01-ebea-8e7d72447119"
      },
      "source": [
        "log = LogisticRegression(\n",
        "    C=1, class_weight=None, dual=False, fit_intercept=True,\n",
        "    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
        "    multi_class='auto', n_jobs=None, penalty='l2',\n",
        "    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
        "    warm_start=False\n",
        ")\n",
        "log.fit(X_train_pca, y_train)\n",
        "pred = log.predict(X_test_pca)\n",
        "print(metrics.classification_report(y_test,pred))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.74      1.00      0.85        63\n",
            "           2       0.83      0.45      0.59        11\n",
            "           3       0.83      0.83      0.83         6\n",
            "           4       0.33      1.00      0.50         1\n",
            "           5       1.00      0.50      0.67         2\n",
            "           6       1.00      0.08      0.15        12\n",
            "           9       1.00      0.50      0.67         2\n",
            "          10       0.86      0.67      0.75         9\n",
            "          16       0.00      0.00      0.00         4\n",
            "\n",
            "    accuracy                           0.75       110\n",
            "   macro avg       0.73      0.56      0.56       110\n",
            "weighted avg       0.77      0.75      0.70       110\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pByKeHfLnnl",
        "outputId": "c2079d44-ad69-430b-c805-8250cb5d659b"
      },
      "source": [
        "print(metrics.f1_score(y_test, pred, average='weighted'))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6988049443931796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ylQarxcMWJf"
      },
      "source": [
        "### 5.3 Las losowy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxatytwdLpft",
        "outputId": "1222c0a5-d34b-4ce5-8ef0-10651e0659b7"
      },
      "source": [
        "rf_clf = RandomForestClassifier(\n",
        "    bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
        "    criterion='gini', max_depth=15, max_features=33,\n",
        "    max_leaf_nodes=None, max_samples=None,\n",
        "    min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "    min_samples_leaf=2, min_samples_split=11,\n",
        "    min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
        "    n_jobs=None, oob_score=False, random_state=None,\n",
        "    verbose=0, warm_start=False).fit(X_train_pca,y_train)\n",
        "\n",
        "pred = rf_clf.predict(X_test_pca)    \n",
        "print(metrics.classification_report(y_test,pred))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.65      0.94      0.77        63\n",
            "           2       0.33      0.09      0.14        11\n",
            "           3       0.75      0.50      0.60         6\n",
            "           4       1.00      1.00      1.00         1\n",
            "           5       0.00      0.00      0.00         2\n",
            "           6       0.00      0.00      0.00        12\n",
            "           9       1.00      0.50      0.67         2\n",
            "          10       0.40      0.44      0.42         9\n",
            "          16       0.00      0.00      0.00         4\n",
            "\n",
            "    accuracy                           0.63       110\n",
            "   macro avg       0.46      0.39      0.40       110\n",
            "weighted avg       0.51      0.63      0.54       110\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JX_NbYCMsTd",
        "outputId": "99c9ead3-12f9-4437-d6f8-3a8a8cdbfbdc"
      },
      "source": [
        "print(metrics.f1_score(y_test, pred, average='weighted'))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5415178441972701\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FegTijKnMbOb"
      },
      "source": [
        "### 5.4 SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yrmwy1yPMjDm",
        "outputId": "f24f22fd-a3d8-458e-94ad-aba5143c6785"
      },
      "source": [
        "svc_clf = SVC(\n",
        "    C=0.5, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
        "    decision_function_shape='ovr', degree=3, gamma=0.001, kernel='linear',\n",
        "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
        "    tol=0.001, verbose=False).fit(X_train_pca, y_train)\n",
        "    \n",
        "pred = svc_clf.predict(X_test_pca)    \n",
        "print(metrics.classification_report(y_test,pred))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.72      1.00      0.84        63\n",
            "           2       1.00      0.27      0.43        11\n",
            "           3       0.71      0.83      0.77         6\n",
            "           4       0.33      1.00      0.50         1\n",
            "           5       1.00      0.50      0.67         2\n",
            "           6       0.00      0.00      0.00        12\n",
            "           9       1.00      1.00      1.00         2\n",
            "          10       0.86      0.67      0.75         9\n",
            "          16       0.00      0.00      0.00         4\n",
            "\n",
            "    accuracy                           0.74       110\n",
            "   macro avg       0.63      0.59      0.55       110\n",
            "weighted avg       0.66      0.74      0.66       110\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcFuBbQqM_Z3",
        "outputId": "498eaa19-cb32-4f3a-bade-0de9d1d7e92a"
      },
      "source": [
        "print(metrics.f1_score(y_test, pred, average='weighted'))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6621182151182151\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXxeO9Y1Mjf9"
      },
      "source": [
        "### 5.5 Liniowy SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tftFJzuxMmdi",
        "outputId": "2e5bc820-4bdf-41bb-903b-1e139457a256"
      },
      "source": [
        "lin_clf = LinearSVC(\n",
        "    C=0.5, class_weight=None, dual=True, fit_intercept=True,\n",
        "    intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
        "    multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
        "    verbose=0).fit(X_train_pca, y_train)\n",
        "    \n",
        "pred = lin_clf.predict(X_test_pca)    \n",
        "print(metrics.classification_report(y_test,pred))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.74      0.97      0.84        63\n",
            "           2       1.00      0.45      0.62        11\n",
            "           3       0.71      0.83      0.77         6\n",
            "           4       0.25      1.00      0.40         1\n",
            "           5       1.00      0.50      0.67         2\n",
            "           6       0.00      0.00      0.00        12\n",
            "           9       1.00      0.50      0.67         2\n",
            "          10       0.60      0.67      0.63         9\n",
            "          16       0.00      0.00      0.00         4\n",
            "\n",
            "    accuracy                           0.73       110\n",
            "   macro avg       0.59      0.55      0.51       110\n",
            "weighted avg       0.65      0.73      0.67       110\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usH1SNZWNHJx",
        "outputId": "6186e73a-50e4-4d3b-c0a6-e4fd0d555c07"
      },
      "source": [
        "print(metrics.f1_score(y_test, pred, average='weighted'))"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6658923487281019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCFbQEfGNIbA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7ghrGZyUld0"
      },
      "source": [
        "## 6. Klasyfikacja po redukcji danych"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bu7yV_6AUpZ2"
      },
      "source": [
        "### 6.1 KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dXTRmzaUoEz",
        "outputId": "3f8124cd-69b3-4482-dfe0-7808805260ad"
      },
      "source": [
        "param_grid={'weights':['distance', 'uniform'], 'n_neighbors':range(1,100)}\n",
        "\n",
        "grid_search = GridSearchCV( KNeighborsClassifier(),param_grid, cv = 10)\n",
        "grid_search.fit(X_train_pca, y_train)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 7 members, which is less than n_splits=10.\n",
            "  % (min_groups, self.n_splits)), UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=10, error_score=nan,\n",
              "             estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30,\n",
              "                                            metric='minkowski',\n",
              "                                            metric_params=None, n_jobs=None,\n",
              "                                            n_neighbors=5, p=2,\n",
              "                                            weights='uniform'),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'n_neighbors': range(1, 100),\n",
              "                         'weights': ['distance', 'uniform']},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUJ9E_JOU_Hj",
        "outputId": "b7f966a6-bb78-4903-8cfb-dc9301bc1db7"
      },
      "source": [
        "print('Best config{}'.format(grid_search.best_estimator_))\n",
        "print('Best score {:.3f}'.format(grid_search.best_score_))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best configKNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
            "                     metric_params=None, n_jobs=None, n_neighbors=6, p=2,\n",
            "                     weights='distance')\n",
            "Best score 0.610\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slXw7M5sU2LN",
        "outputId": "02ca6a88-bbb1-49c0-886e-555a7f5e24e2"
      },
      "source": [
        "knn_clf = KNeighborsClassifier(\n",
        "    algorithm='auto', leaf_size=30, metric='minkowski',\n",
        "    metric_params=None, n_jobs=None, n_neighbors=6, p=2,\n",
        "    weights='distance')\n",
        "knn_clf.fit(X_train_pca,y_train)\n",
        "\n",
        "pred = knn_clf.predict(X_test_pca)\n",
        "print(metrics.classification_report(y_test,pred))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.62      0.95      0.75        63\n",
            "           2       0.00      0.00      0.00        11\n",
            "           3       1.00      0.67      0.80         6\n",
            "           4       0.50      1.00      0.67         1\n",
            "           5       1.00      0.50      0.67         2\n",
            "           6       0.33      0.08      0.13        12\n",
            "           9       0.00      0.00      0.00         2\n",
            "          10       1.00      0.22      0.36         9\n",
            "          16       0.00      0.00      0.00         4\n",
            "\n",
            "    accuracy                           0.63       110\n",
            "   macro avg       0.49      0.38      0.38       110\n",
            "weighted avg       0.55      0.63      0.54       110\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKznCiPWVIbG",
        "outputId": "b122df98-628a-463a-d389-5639d8111912"
      },
      "source": [
        "print(metrics.f1_score(y_test, pred, average='weighted'))"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5356611570247933\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA4tiQvTVYNt"
      },
      "source": [
        "### 6.2 Regresja logistyczna"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WE9bTYo5VK3W",
        "outputId": "86129bb5-cba1-4228-b37c-ce6759713617"
      },
      "source": [
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
        "\n",
        "grid_search_log = GridSearchCV(LogisticRegression(penalty='l2'), param_grid, cv=5)\n",
        "grid_search_log.fit(X_train_pca, y_train)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                          fit_intercept=True,\n",
              "                                          intercept_scaling=1, l1_ratio=None,\n",
              "                                          max_iter=100, multi_class='auto',\n",
              "                                          n_jobs=None, penalty='l2',\n",
              "                                          random_state=None, solver='lbfgs',\n",
              "                                          tol=0.0001, verbose=0,\n",
              "                                          warm_start=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWKPztAPVlz5",
        "outputId": "a0d3a06a-0d6f-4277-a402-0eb21187b17a"
      },
      "source": [
        "print('Best config{}'.format(grid_search_log.best_estimator_))\n",
        "print('Best score {:.3f}'.format(grid_search_log.best_score_))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best configLogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
            "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
            "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
            "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
            "                   warm_start=False)\n",
            "Best score 0.723\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8b86sZXVuqn",
        "outputId": "80cb63a1-d9c6-487f-8bb0-c6511c5c526c"
      },
      "source": [
        "log = LogisticRegression(\n",
        "    C=1, class_weight=None, dual=False, fit_intercept=True,\n",
        "    intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
        "    multi_class='auto', n_jobs=None, penalty='l2',\n",
        "    random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
        "    warm_start=False)\n",
        "log.fit(X_train_pca, y_train)\n",
        "\n",
        "pred = log.predict(X_test_pca)\n",
        "print(metrics.classification_report(y_test,pred))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.74      1.00      0.85        63\n",
            "           2       0.83      0.45      0.59        11\n",
            "           3       0.83      0.83      0.83         6\n",
            "           4       0.33      1.00      0.50         1\n",
            "           5       1.00      0.50      0.67         2\n",
            "           6       1.00      0.08      0.15        12\n",
            "           9       1.00      0.50      0.67         2\n",
            "          10       0.86      0.67      0.75         9\n",
            "          16       0.00      0.00      0.00         4\n",
            "\n",
            "    accuracy                           0.75       110\n",
            "   macro avg       0.73      0.56      0.56       110\n",
            "weighted avg       0.77      0.75      0.70       110\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1kj0TQyVyor",
        "outputId": "2f9a3b82-9927-485d-c525-15c76be175fb"
      },
      "source": [
        "print(metrics.f1_score(y_test, pred, average='weighted'))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6988049443931796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ES-GUDtoV7R2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLJB2pTDWAAx"
      },
      "source": [
        "### 6.3 Las losowy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6hdIiEhWF0N",
        "outputId": "5817678e-b44d-4a6c-a96a-38c16b05e6d2"
      },
      "source": [
        "param_grid = {\"max_depth\": range(1,20),\n",
        "              \"max_features\": sp_randint(1, 40),\n",
        "              \"min_samples_split\": sp_randint(2, 30),\n",
        "              \"min_samples_leaf\": sp_randint(1, 20),\n",
        "              \"bootstrap\": [True, False]}\n",
        "grid_search_RF = RandomizedSearchCV(RandomForestClassifier(n_estimators=1000), param_distributions=param_grid,\n",
        "                                   n_iter=30, random_state=0,n_jobs=-1)\n",
        "grid_search_RF.fit(X_train_pca, y_train)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=None, error_score=nan,\n",
              "                   estimator=RandomForestClassifier(bootstrap=True,\n",
              "                                                    ccp_alpha=0.0,\n",
              "                                                    class_weight=None,\n",
              "                                                    criterion='gini',\n",
              "                                                    max_depth=None,\n",
              "                                                    max_features='auto',\n",
              "                                                    max_leaf_nodes=None,\n",
              "                                                    max_samples=None,\n",
              "                                                    min_impurity_decrease=0.0,\n",
              "                                                    min_impurity_split=None,\n",
              "                                                    min_samples_leaf=1,\n",
              "                                                    min_samples_split=2,\n",
              "                                                    min_weight_fraction_leaf=0.0,\n",
              "                                                    n_estimators=1000,\n",
              "                                                    n_...\n",
              "                                        'max_features': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fb4e9e141d0>,\n",
              "                                        'min_samples_leaf': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fb4ecc7e690>,\n",
              "                                        'min_samples_split': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7fb4e9e14e50>},\n",
              "                   pre_dispatch='2*n_jobs', random_state=0, refit=True,\n",
              "                   return_train_score=False, scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ON5HkwcWRBc",
        "outputId": "968edff8-91d3-4ab4-b597-e0e4f8125efb"
      },
      "source": [
        "print('Best config{}'.format(grid_search_RF.best_estimator_))\n",
        "print('Best score {:.3f}'.format(grid_search_RF.best_score_))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best configRandomForestClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
            "                       criterion='gini', max_depth=10, max_features=36,\n",
            "                       max_leaf_nodes=None, max_samples=None,\n",
            "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
            "                       min_samples_leaf=1, min_samples_split=7,\n",
            "                       min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
            "                       n_jobs=None, oob_score=False, random_state=None,\n",
            "                       verbose=0, warm_start=False)\n",
            "Best score 0.631\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGNBY69_WhhT",
        "outputId": "f76cc809-e158-48b7-f9d1-c1b535ed5a51"
      },
      "source": [
        "rf_clf = RandomForestClassifier(\n",
        "    bootstrap=False, ccp_alpha=0.0, class_weight=None,\n",
        "    criterion='gini', max_depth=10, max_features=36,\n",
        "    max_leaf_nodes=None, max_samples=None,\n",
        "    min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "    min_samples_leaf=1, min_samples_split=7,\n",
        "    min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
        "    n_jobs=None, oob_score=False, random_state=None,\n",
        "    verbose=0, warm_start=False).fit(X_train_pca,y_train)\n",
        "\n",
        "pred = rf_clf.predict(X_test_pca)    \n",
        "print(metrics.classification_report(y_test,pred))"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.65      0.95      0.77        63\n",
            "           2       0.33      0.09      0.14        11\n",
            "           3       0.75      0.50      0.60         6\n",
            "           4       1.00      1.00      1.00         1\n",
            "           5       0.00      0.00      0.00         2\n",
            "           6       0.00      0.00      0.00        12\n",
            "           9       1.00      0.50      0.67         2\n",
            "          10       0.44      0.44      0.44         9\n",
            "          16       0.00      0.00      0.00         4\n",
            "\n",
            "    accuracy                           0.64       110\n",
            "   macro avg       0.46      0.39      0.40       110\n",
            "weighted avg       0.51      0.64      0.55       110\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdIDbXFYWiS6",
        "outputId": "dde108cf-55b6-4ef4-ddc0-3d4bb8322a2b"
      },
      "source": [
        "print(metrics.f1_score(y_test, pred, average='weighted'))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5479905041195364\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZC9_W69XOAB"
      },
      "source": [
        "### 6.4 SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSZ-K8sxXM_u",
        "outputId": "da7c30bb-adff-4eab-a403-16049459a751"
      },
      "source": [
        "svc = SVC()\n",
        "param_grid = {'kernel':('rbf', 'sigmoid','linear'),\n",
        "              'C':[0.001, 0.01, 0.1, 0.5, 1, 10, 50, 100, 1000],\n",
        "              'gamma':[0.001, 0.01, 0.1, 0.5, 1, 10]}\n",
        "\n",
        "grid_search_svc = GridSearchCV(svc, param_grid, cv = 5)\n",
        "grid_search_svc.fit(X_train_pca, y_train)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=SVC(C=1.0, break_ties=False, cache_size=200,\n",
              "                           class_weight=None, coef0=0.0,\n",
              "                           decision_function_shape='ovr', degree=3,\n",
              "                           gamma='scale', kernel='rbf', max_iter=-1,\n",
              "                           probability=False, random_state=None, shrinking=True,\n",
              "                           tol=0.001, verbose=False),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'C': [0.001, 0.01, 0.1, 0.5, 1, 10, 50, 100, 1000],\n",
              "                         'gamma': [0.001, 0.01, 0.1, 0.5, 1, 10],\n",
              "                         'kernel': ('rbf', 'sigmoid', 'linear')},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kij9lpfGXZo8",
        "outputId": "3b541ee7-a721-425b-c9eb-507a48cae845"
      },
      "source": [
        "print('Best config{}'.format(grid_search_svc.best_estimator_))\n",
        "print('Best score {:.3f}'.format(grid_search_svc.best_score_))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best configSVC(C=50, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
            "    decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',\n",
            "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
            "    tol=0.001, verbose=False)\n",
            "Best score 0.717\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zaFvyD_6XdHf",
        "outputId": "78c224d2-10c0-4286-be40-d15f4cfb967e"
      },
      "source": [
        "svc_clf = SVC(\n",
        "    C=50, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
        "    decision_function_shape='ovr', degree=3, gamma=0.01, kernel='rbf',\n",
        "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
        "    tol=0.001, verbose=False).fit(X_train_pca, y_train)\n",
        "    \n",
        "pred = svc_clf.predict(X_test_pca)    \n",
        "print(metrics.classification_report(y_test,pred))"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.72      1.00      0.84        63\n",
            "           2       1.00      0.27      0.43        11\n",
            "           3       0.71      0.83      0.77         6\n",
            "           4       0.33      1.00      0.50         1\n",
            "           5       1.00      0.50      0.67         2\n",
            "           6       0.00      0.00      0.00        12\n",
            "           9       1.00      1.00      1.00         2\n",
            "          10       0.86      0.67      0.75         9\n",
            "          16       0.00      0.00      0.00         4\n",
            "\n",
            "    accuracy                           0.74       110\n",
            "   macro avg       0.63      0.59      0.55       110\n",
            "weighted avg       0.66      0.74      0.66       110\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kitOgUopXf3K",
        "outputId": "6296462c-66f1-48f8-d605-f03bcdb35480"
      },
      "source": [
        "print(metrics.f1_score(y_test, pred, average='weighted'))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6621182151182151\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Vr1TWwyXis2"
      },
      "source": [
        "### 6.4 Liniowe SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOSNJUcIXkZI",
        "outputId": "329ba878-6885-46a6-eb9b-87c0abbc520b"
      },
      "source": [
        "lin = LinearSVC()\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 0.5, 1, 10, 50, 100, 1000],\n",
        "    'max_iter':[1000,10000]}\n",
        "\n",
        "grid_search_lsvc = GridSearchCV(lin, param_grid, cv = 5)\n",
        "grid_search_lsvc.fit(X_train_pca, y_train)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  \"the number of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score=nan,\n",
              "             estimator=LinearSVC(C=1.0, class_weight=None, dual=True,\n",
              "                                 fit_intercept=True, intercept_scaling=1,\n",
              "                                 loss='squared_hinge', max_iter=1000,\n",
              "                                 multi_class='ovr', penalty='l2',\n",
              "                                 random_state=None, tol=0.0001, verbose=0),\n",
              "             iid='deprecated', n_jobs=None,\n",
              "             param_grid={'C': [0.001, 0.01, 0.1, 0.5, 1, 10, 50, 100, 1000],\n",
              "                         'max_iter': [1000, 10000]},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QV_x4_xXoim",
        "outputId": "45cd003b-ff80-4d8f-ab69-145120f9b287"
      },
      "source": [
        "print('Best config{}'.format(grid_search_lsvc.best_estimator_))\n",
        "print('Best score {:.3f}'.format(grid_search_lsvc.best_score_))"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best configLinearSVC(C=0.1, class_weight=None, dual=True, fit_intercept=True,\n",
            "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
            "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
            "          verbose=0)\n",
            "Best score 0.726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V75aEge-Xqcd",
        "outputId": "8edbb81b-6385-4fae-8f35-857b449e9faa"
      },
      "source": [
        "lin_clf = LinearSVC(\n",
        "    C=0.1, class_weight=None, dual=True, fit_intercept=True,\n",
        "    intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
        "    multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
        "    verbose=0).fit(X_train_pca, y_train)\n",
        "    \n",
        "pred = lin_clf.predict(X_test_pca)    \n",
        "print(metrics.classification_report(y_test,pred))"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1       0.74      1.00      0.85        63\n",
            "           2       1.00      0.45      0.62        11\n",
            "           3       0.71      0.83      0.77         6\n",
            "           4       0.33      1.00      0.50         1\n",
            "           5       0.00      0.00      0.00         2\n",
            "           6       0.00      0.00      0.00        12\n",
            "           9       1.00      0.50      0.67         2\n",
            "          10       0.67      0.67      0.67         9\n",
            "          16       0.00      0.00      0.00         4\n",
            "\n",
            "    accuracy                           0.74       110\n",
            "   macro avg       0.50      0.49      0.45       110\n",
            "weighted avg       0.64      0.74      0.66       110\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtU54ZzkXuPC",
        "outputId": "ee89b1cc-d139-4257-b50c-22489f2c8518"
      },
      "source": [
        "print(metrics.f1_score(y_test, pred, average='weighted'))"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6632623007623007\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6QuGfoCa6Ey"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}